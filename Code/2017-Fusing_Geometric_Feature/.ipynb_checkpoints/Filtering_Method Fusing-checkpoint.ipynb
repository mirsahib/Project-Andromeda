{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "- Removing Constant\n",
    "- Removing Quasi Constant\n",
    "- Removing Duplicate Feature\n",
    "- Removing Correlated Feature\n",
    "- Performing LDA and PCA\n",
    "\n",
    "## What we have done\n",
    "\n",
    "We remove constant feature which have low variance\n",
    "We remove Quasi-constant are the features that are almost constant. In other words, these features have the same values for a very large subset of the outputs. Such features are not very useful for making predictions. There is no rule as to what should be the threshold for the variance of quasi-constant features.But in this project we used 0.01 as the threshold value.\n",
    "We remove two or more than two features which are mutually correlated because the convey redundant information to the model and hence only one of the correlated feature should be retained to reduce the number of features.\n",
    "\n",
    "We perform LDA and PCA for feature reduction\n",
    "\n",
    "After performing the feature selection method we divide the data into 5 fold and for each fold we split the fold into train and test data set (80/20)ratio\n",
    "\n",
    "Finally We run Random forest,SVM,Decision Tree and KNN classifier the selected feature\n",
    "\n",
    "#### Short Description of LDA and PCA\n",
    "\n",
    "LDA is a supervised data compression technique which is aimed increasing class distinction techniques.\n",
    "The general concept behind LDA is very similar to PCA ,while PCA attempts to find the orthogonal component axes of maximum variance in a data-set, the goal in LDA is to find the feature subspace that optimizes class separability and to serve this purpose it requires the class labels.\n",
    "PCA is an unsupervised linear transformation technique.\n",
    "PCA helps us to identify patterns in data based on the correlation betweeen features. In a nutshell , PCA aims at finding the directions of maximum variance in high-dimensional data and projects it onto a new subspace of lower or equal number of dimensions than original feature space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,location,numOfFold):\n",
    "        self.fold = numOfFold\n",
    "        self.kFold = KFold(numOfFold,True,1)\n",
    "        self.avg_accuracy = []\n",
    "        self.data = pd.read_csv(location)\n",
    "        self.data = self.data.fillna(self.data.mean())\n",
    "        self.X = self.data.drop('label',axis=1)\n",
    "        self.Y = self.data['label']\n",
    "        print('X shape:',str(self.X.shape))\n",
    "        print('Y shape:',str(self.Y.shape))\n",
    "    def loadData(self,location,numOfFold):\n",
    "        self.fold = numOfFold\n",
    "        self.kFold = KFold(numOfFold,True,1)\n",
    "        self.avg_accuracy = []\n",
    "        self.data = pd.read_csv(location)\n",
    "        self.data = self.data.fillna(self.data.mean())\n",
    "        self.X = self.data.drop('label',axis=1)\n",
    "        self.Y = self.data['label']\n",
    "        print('X shape:',str(self.X.shape))\n",
    "        print('Y shape:',str(self.Y.shape))\n",
    "    def removeContantFeature(self):\n",
    "        #print('Removing constant feature')\n",
    "        constant_filter = VarianceThreshold(threshold=0)\n",
    "        constant_filter.fit(self.X_train)\n",
    "        #print('Number of constant feature ',constant_filter.get_support().sum())\n",
    "        constant_list = [not temp for temp in constant_filter.get_support()]\n",
    "        self.X.columns[constant_list]\n",
    "        self.X_train_filter = constant_filter.transform(self.X_train)\n",
    "        self.X_test_filter = constant_filter.transform(self.X_test)\n",
    "        #print('Shape of the dataset after removal of constant features')\n",
    "        #print(self.X_train_filter.shape,self.X_test_filter.shape,self.X_train.shape,'\\n')\n",
    "    def removeQuasiConstant(self):\n",
    "        #print('Removing Quasi constant feature')\n",
    "        quasi_constant_filter = VarianceThreshold(threshold = 0.01)\n",
    "        quasi_constant_filter.fit(self.X_train_filter)\n",
    "        #print('Number of quasi constant feature ',quasi_constant_filter.get_support().sum())\n",
    "        self.X_train_quasi_filter = quasi_constant_filter.transform(self.X_train_filter)\n",
    "        self.X_test_quasi_filter = quasi_constant_filter.transform(self.X_test_filter)\n",
    "        #print('Shape of the dataset after removal of quasi constant features')\n",
    "        #print(self.X_train_quasi_filter.shape,self.X_test_quasi_filter.shape,self.X_train.shape,'\\n')\n",
    "        \n",
    "    def removeDuplicateFeature(self):\n",
    "        X_train_T = self.X_train_quasi_filter.T\n",
    "        X_test_T = self.X_test_quasi_filter.T\n",
    "        X_train_T = pd.DataFrame(X_train_T)\n",
    "        X_test_T = pd.DataFrame(X_test_T)\n",
    "        #print('Number of duplicate feature ',X_train_T.duplicated().sum())\n",
    "        duplicated_feature = X_train_T.duplicated()\n",
    "        features_to_keep = [not index for index in duplicated_feature]\n",
    "        self.X_train_unique = X_train_T[features_to_keep].T\n",
    "        self.X_test_unique = X_test_T[features_to_keep].T\n",
    "        #print('Shape of the dataset after removal of duplicate features')\n",
    "        #print(self.X_train_unique.shape,self.X_test_unique.shape,self.X_train.shape,'\\n')\n",
    "    def get_correlation(self,data, threshold):\n",
    "        corr_col = set()\n",
    "        corrmat = data.corr()\n",
    "        for i in range(len(corrmat.columns)):\n",
    "            for j in range(i):\n",
    "                if abs(corrmat.iloc[i, j])> threshold:\n",
    "                    colname = corrmat.columns[i]\n",
    "                    corr_col.add(colname)\n",
    "        return corr_col\n",
    "    def removeCorrelatedFeature(self):\n",
    "        corrmat = self.X_train_unique.corr()\n",
    "        corr_features = self.get_correlation(self.X_train_unique, 0.85)\n",
    "        self.X_train_uncorr = self.X_train_unique.drop(labels=corr_features, axis = 1)\n",
    "        self.X_test_uncorr = self.X_test_unique.drop(labels = corr_features, axis = 1)\n",
    "        #print('Shape of the dataset after removal of correlated features')\n",
    "        #print(self.X_train_uncorr.shape,self.X_test_uncorr.shape,self.X_train.shape)\n",
    "\n",
    "    def runRandomForest(self,corrParm):#invoke corrParm to remove correlated feature\n",
    "        count = 1\n",
    "        for train_index,test_index in self.kFold.split(self.data):\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.X.iloc[train_index], self.X.iloc[test_index],self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "            #print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "            self.removeContantFeature()\n",
    "            self.removeQuasiConstant()\n",
    "            self.removeDuplicateFeature()\n",
    "            if corrParm == 'Y':\n",
    "                self.removeCorrelatedFeature()\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "                clf.fit(self.X_train_unique, self.y_train)\n",
    "                self.y_pred = clf.predict(self.X_test_unique)\n",
    "            else:\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "                clf.fit(self.X_train_unique, self.y_train)\n",
    "                self.y_pred = clf.predict(self.X_test_unique)\n",
    "\n",
    "            accuracy = accuracy_score(self.y_test, self.y_pred)*100\n",
    "            print('Accuracy of fold ',str(count),': ',accuracy)\n",
    "            self.avg_accuracy.append(accuracy)\n",
    "            count = count+1\n",
    "        accDF = pd.DataFrame(self.avg_accuracy,columns = ['Accuracy per fold'],index = None)\n",
    "        print(accDF)\n",
    "        print('Average accuracy of Random forest ', sum(self.avg_accuracy)/self.fold)\n",
    "            \n",
    "        return\n",
    "    def runSVM(kernelTrick):\n",
    "        count = 1\n",
    "        scaler = StandardScaler()\n",
    "        for train_index,test_index in self.kFold.split(self.data):\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.X.iloc[train_index], self.X.iloc[test_index],self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "            #print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "            self.removeContantFeature()\n",
    "            self.removeQuasiConstant()\n",
    "            self.removeDuplicateFeature()\n",
    "            X_train_scaled = scaler.fit_transform(self.X_train_unique)\n",
    "            X_test_scaled = scaler.fit_transform(self.X_test_unique)\n",
    "            clf = SVC(kernel = kernelTrick , C = 1)\n",
    "            clf.fit(self.X_train_scaled, self.y_train)\n",
    "            self.y_pred = clf.predict(self.X_test_scaled)\n",
    "            accuracy = accuracy_score(self.y_test, self.y_pred)*100\n",
    "            #print('Accuracy of fold ',str(count),': ',accuracy)\n",
    "            self.avg_accuracy.append(accuracy)\n",
    "            count = count+1\n",
    "        accDF = pd.DataFrame(self.avg_accuracy,columns = ['Accuracy per fold'],index = None)\n",
    "        print(accDF)\n",
    "        print('Average accuracy of SVM with',kernelTrick,' : ', sum(self.avg_accuracy)/self.fold)\n",
    "    def runDecisionTree(self,Criterion,corrParm):\n",
    "        count = 1\n",
    "        for train_index,test_index in self.kFold.split(self.data):\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.X.iloc[train_index], self.X.iloc[test_index],self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "            #print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "            self.removeContantFeature()\n",
    "            self.removeQuasiConstant()\n",
    "            self.removeDuplicateFeature()\n",
    "            self.removeCorrelatedFeature()\n",
    "            if corrParm == 'Y':\n",
    "                self.removeCorrelatedFeature()\n",
    "                clf = DecisionTreeClassifier(criterion = Criterion, random_state = 100,\n",
    "                               max_depth=30, min_samples_leaf=5)\n",
    "                clf.fit(self.X_train_unique, self.y_train)\n",
    "                self.y_pred = clf.predict(self.X_test_unique)\n",
    "            else:\n",
    "                clf = DecisionTreeClassifier(criterion = Criterion, random_state = 100,\n",
    "                               max_depth=30, min_samples_leaf=5)\n",
    "                clf.fit(self.X_train_unique, self.y_train)\n",
    "                self.y_pred = clf.predict(self.X_test_unique)\n",
    "\n",
    "\n",
    "            accuracy = accuracy_score(self.y_test, self.y_pred)*100\n",
    "            #print('Accuracy of fold ',str(count),': ',accuracy)\n",
    "            self.avg_accuracy.append(accuracy)\n",
    "            count = count+1\n",
    "        accDF = pd.DataFrame(self.avg_accuracy,columns = ['Accuracy per fold'],index = None)\n",
    "        print(accDF)\n",
    "        print('Average accuracy of Decision Tree with ',Criterion,' as criterion: ', sum(self.avg_accuracy)/self.fold)\n",
    "    def runKNNClassifier(self,neighbor,corrParm):\n",
    "        count = 1\n",
    "        for train_index,test_index in self.kFold.split(self.data):\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.X.iloc[train_index], self.X.iloc[test_index],self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "            #print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "            self.removeContantFeature()\n",
    "            self.removeQuasiConstant()\n",
    "            self.removeDuplicateFeature()\n",
    "            if corrParm == 'Y':\n",
    "                self.removeCorrelatedFeature()\n",
    "                clf = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "                clf.fit(self.X_train_unique, self.y_train)\n",
    "                self.y_pred = clf.predict(self.X_test_unique)\n",
    "            else:\n",
    "                clf = KNeighborsClassifier(n_neighbors=neighbor)\n",
    "                clf.fit(self.X_train_unique, self.y_train)\n",
    "                self.y_pred = clf.predict(self.X_test_unique)\n",
    "\n",
    "            accuracy = accuracy_score(self.y_test, self.y_pred)*100\n",
    "            #print('Accuracy of fold ',str(count),': ',accuracy)\n",
    "            self.avg_accuracy.append(accuracy)\n",
    "            count = count+1\n",
    "        accDF = pd.DataFrame(self.avg_accuracy,columns = ['Accuracy per fold'],index = None)\n",
    "        print(accDF)\n",
    "        print('Average accuracy of KNN Classifier', sum(self.avg_accuracy)/self.fold)\n",
    "    def runLDA(self,corrParm):\n",
    "        count=0\n",
    "        for train_index,test_index in self.kFold.split(self.data):\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.X.iloc[train_index], self.X.iloc[test_index],self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "            #print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "            self.removeContantFeature()\n",
    "            self.removeQuasiConstant()\n",
    "            self.removeDuplicateFeature()\n",
    "            if corrParm == 'Y':\n",
    "                self.removeCorrelatedFeature()\n",
    "                lda = LDA(n_components=1)\n",
    "                X_train_lda = lda.fit_transform(self.X_train_uncorr, self.y_train)\n",
    "                X_test_lda = lda.transform(self.X_test_uncorr)\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "                clf.fit(X_train_lda, self.y_train)\n",
    "                self.y_pred = clf.predict(X_test_lda)\n",
    "            else:\n",
    "                lda = LDA(n_components=1)\n",
    "                X_train_lda = lda.fit_transform(self.X_train_unique, self.y_train)\n",
    "                X_test_lda = lda.transform(self.X_test_unique)\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "                clf.fit(X_train_lda, self.y_train)\n",
    "                self.y_pred = clf.predict(X_test_lda)\n",
    "            accuracy = accuracy_score(self.y_test, self.y_pred)*100\n",
    "            #print('Accuracy of fold ',str(count),': ',accuracy)\n",
    "            self.avg_accuracy.append(accuracy)\n",
    "            count = count+1\n",
    "        accDF = pd.DataFrame(self.avg_accuracy,columns = ['Accuracy per fold'],index = None)\n",
    "        print(accDF)\n",
    "        print('Average accuracy of running LDA', sum(self.avg_accuracy)/self.fold)\n",
    "\n",
    "    def runPCA(self,corrParm):\n",
    "        count=0\n",
    "        for train_index,test_index in self.kFold.split(self.data):\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = self.X.iloc[train_index], self.X.iloc[test_index],self.Y.iloc[train_index], self.Y.iloc[test_index]\n",
    "            #print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "            self.removeContantFeature()\n",
    "            self.removeQuasiConstant()\n",
    "            self.removeDuplicateFeature()\n",
    "            if corrParm == 'Y':\n",
    "                self.removeCorrelatedFeature()\n",
    "                pca = PCA(n_components=2, random_state=42)\n",
    "                pca.fit(self.X_train_uncorr)\n",
    "                X_train_pca = pca.transform(self.X_train_uncorr)\n",
    "                X_test_pca = pca.transform(self.X_test_uncorr)\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "                clf.fit(X_train_pca, self.y_train)\n",
    "                self.y_pred = clf.predict(X_test_pca)\n",
    "            else:\n",
    "                pca = PCA(n_components=2, random_state=42)\n",
    "                pca.fit(self.X_train_unique)\n",
    "                X_train_pca = pca.transform(self.X_train_unique)\n",
    "                X_test_pca = pca.transform(self.X_test_unique)\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "                clf.fit(X_train_pca, self.y_train)\n",
    "                self.y_pred = clf.predict(X_test_pca)\n",
    "            accuracy = accuracy_score(self.y_test, self.y_pred)*100\n",
    "            #print('Accuracy of fold ',str(count),': ',accuracy)\n",
    "            self.avg_accuracy.append(accuracy)\n",
    "            count = count+1\n",
    "        accDF = pd.DataFrame(self.avg_accuracy,columns = ['Accuracy per fold'],index = None)\n",
    "        print(accDF)\n",
    "        print('Average accuracy of running LDA', sum(self.avg_accuracy)/self.fold)\n",
    "\n",
    "    def showData(self):\n",
    "        return self.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (22797, 141)\n",
      "Y shape: (22797,)\n"
     ]
    }
   ],
   "source": [
    "location = r'/home/mirsahib/Desktop/Project-Andromeda/Dataset/Fusing_Geometric_Feature_Extracted/fusing_geometric.csv'\n",
    "FilterModel = Model(location,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of fold  1 :  89.51754385964912\n"
     ]
    }
   ],
   "source": [
    "FilterModel.runRandomForest('N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runDecisionTree('gini','N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runDecisionTree('entropy','N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runKNNClassifier(4,'N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runKNNClassifier(3,'N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (After removing correlated data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runLDA('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (before removing correlated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runLDA('N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (before removing correlated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runPCA('N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (After removing correlated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterModel.loadData(location,5)\n",
    "FilterModel.runPCA('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
